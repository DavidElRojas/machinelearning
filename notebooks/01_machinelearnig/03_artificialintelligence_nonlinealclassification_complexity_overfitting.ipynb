{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<a href=\"https://colab.research.google.com/github/marcoteran/machinelearning/blob/master/notebooks/01_machinelearning/03_artificialintelligence_nonlinealclassification_complexity_overfitting.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de código\n",
    "# Sesión 03: Clasificación no lineal, complejidad y sobreajuste\n",
    "## Inteligencia Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran **E-mail:** marco.tulio.teran@gmail.com,\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos primero unas librerías y funciones que vamos a usar a durante la sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar un conjunto de datos en 2D\n",
    "def plot_data(X, y):\n",
    "    y_unique = np.unique(y)\n",
    "    colors = pl.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n",
    "    for this_y, color in zip(y_unique, colors):\n",
    "        this_X = X[y == this_y]\n",
    "        pl.scatter(this_X[:, 0], this_X[:, 1],  c=color.reshape(1,-1),\n",
    "                    alpha=0.5, edgecolor='k',\n",
    "                    label=\"Class %s\" % this_y)\n",
    "    pl.legend(loc=\"best\")\n",
    "    pl.title(\"Data\")\n",
    "    \n",
    "# Función para visualizar de la superficie de decisión de un clasificador\n",
    "def plot_decision_region(X, pred_fun):\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "    min_x = min_x - (max_x - min_x) * 0.05\n",
    "    max_x = max_x + (max_x - min_x) * 0.05\n",
    "    min_y = min_y - (max_y - min_y) * 0.05\n",
    "    max_y = max_y + (max_y - min_y) * 0.05\n",
    "    x_vals = np.linspace(min_x, max_x, 100)\n",
    "    y_vals = np.linspace(min_y, max_y, 100)\n",
    "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
    "    grid_r, grid_c = XX.shape\n",
    "    ZZ = np.zeros((grid_r, grid_c))\n",
    "    for i in range(grid_r):\n",
    "        for j in range(grid_c):\n",
    "            ZZ[i, j] = pred_fun(XX[i, j], YY[i, j])\n",
    "    pl.contourf(XX, YY, ZZ, 100, cmap = pl.cm.coolwarm, vmin= -1, vmax=2)\n",
    "    pl.colorbar()\n",
    "    pl.xlabel(\"x\")\n",
    "    pl.ylabel(\"y\")\n",
    "    \n",
    "def gen_pred_fun(clf):\n",
    "    def pred_fun(x1, x2):\n",
    "        x = np.array([[x1, x2]])\n",
    "        return clf.predict(x)[0]\n",
    "    return pred_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complejidad\n",
    "\n",
    "Un modelo de clasificación puede ser tan complejo como para aprenderse de memoria el conjunto de entrenamiento. Esta complejidad está determinada por los parámetros internos del modelo. A continuación, observaremos como se comporta esta complejidad usando un modelo clasificación no lineal como lo es  **k-vecinos más cercanos** (*K-nearest neighbors* en inglés)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del conjunto de datos\n",
    "\n",
    "Vamos a trabajar con un conjunto de datos artificial (conjunto de datos de juguete). El conjunto es creado usando la funcionalidad `make_moons` de Scikit-Learn [ver más](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html).\n",
    "`make_moons` permite introducir algo de ruido sobre las muestras creadas.\n",
    "\n",
    "`sklearn.datasets.make_moons(n_samples=100, *, shuffle=True, noise=None, random_state=None)`\n",
    "* **n_samples** Indica el número de muestras para cada grupo (clúster).\n",
    "* **n_features** El número de características de cada muestra (clústers).\n",
    "* **centros** El número de centros a generar o la posición fija del centro.\n",
    "* **noise** Desviación estándar del ruido gaussiano agregado a las muestras que pertenecen al mismo grupo\n",
    "* **shuffle** Mezcla las muestras\n",
    "* **random_state** Semilla del generador de muestras aleatorias. Este parámetro garantiza reproducibilidad del conjunto de datos.\n",
    "\n",
    "Salidas:\n",
    "* *X:* lista de las muestras generadas `[n_samples, n_features]`\n",
    "* *y:* lista de etiquetas `[n_samples]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=600, noise=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('X ~ n_muestras x n_características:', X.shape)\n",
    "print('y ~ n_muestras:', y.shape)\n",
    "\n",
    "print('\\nPrimeras 5 muestras:\\n', X[:5, :])\n",
    "print('\\nPrimeras 5 etiquetas:', y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))  \n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que es dificil establecer una separación lineal como en regresión logística. Por lo tanto es necesario usar un modelo de clasificación no lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo: K-vecinos más cercanos\n",
    "\n",
    "La clasificación basada en vecinos es un tipo de aprendizaje basado en ejemplos. El modelo almacena los ejemplos vistos durante entrenamiento y clasifica un elemento no visto, usando una simple regla de votación por mayoría. Si se ubica un **punto** en el espacio de características, se le asigna como clase el valor de la clase que tenga la mayor cantidad de ejemplos en la vecindad del punto. Este ejemplo lo podemos ver ilustrado en la imagen:\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/knn.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provee una implementación del algoritmo conocida como `KNeighborsClassifier`. `KNeighborsClassifier` tiene un parámetro $n\\_neighbors$ o $k$, dónde $k$ es un entero definido por el usuario que determina cuantos vecinos evalua para determinar la clase de una instancia nunca antes vista. La elección de este parámetro es definida totalmente por la naturaleza de los datos.\n",
    "\n",
    "Observaremos que dependiendo del valor de vecinos más cercanos, conseguimos diferentes funciones de ajuste, unas más suaves que otras. Vamos a evaluar el efecto del parámetro $k$ en la complejidad del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a extraer un porcentaje al azar del conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(X), int(len(X)*.5), replace=False)\n",
    "X_reduced = X[idx]\n",
    "y_reduced = y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos un modelo `knn` llamando la función `fit()` sobre el conjunto de datos reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_reduced, y_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))\n",
    "plot_decision_region(X_reduced, gen_pred_fun(knn))\n",
    "plot_data(X_reduced, y_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error: {}'.format(1 - knn.score(X_reduced, y_reduced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Tiene sentido que el error sea del $0\\%$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agreguemos los datos que descartamos anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx_c = [i for i in range(len(X)) if i not in idx]\n",
    "X_complement = X[idx_c]\n",
    "y_complement = y[idx_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))   \n",
    "plot_decision_region(X_complement, gen_pred_fun(knn))\n",
    "plot_data(X_complement, y_complement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error: {}'.format(1 - knn.score(X_complement, y_complement)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que cuando el número de vecinos es $1$, el modelo se ajusta demasiado al ruído de los datos de entrada, por lo tanto sufre de **sobreajuste**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error de entrenamiento y generalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de aprendizaje de máquina tiene como objetivo principal hacer predicciones de manera acertada sobre ejemplos nunca antes vistos por el modelo. Esto se conoce como error de generalización. Para poder medir el error de generalización, dividimos el conjunto de datos en dos particiones: \n",
    "\n",
    "* **Entrenamiento**: Se usará para entrenar el modelo.\n",
    "* **Prueba**: Se usará para medir el error de generalización.\n",
    "\n",
    "En la siguiente imagen encontramos una ilustración de cómo se hace un particionamiento en entrenamiento y prueba.\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/train_test_split.svg\" width=\"50%\">\n",
    "\n",
    "Una de las prácticas recomendadas, es particionar los datos $70\\%$ para entrenamiento y $30\\%$ para prueba. Cuando el número de muestras es muy grande ($\\ge 70K$), podemos reducir el porcentaje de muestras para prueba, a $90-10\\%$. Sin embargo, deben hacerse unas aclaraciones sobre la generalización:\n",
    "\n",
    "* El conjunto de prueba debe ser una muestra representativa del conjunto de datos. El muestreo de ejemplos debe hacerse de forma independiente e idénticamente aleatoria de una distribución. Esto quiere decir, que el muestreo de un ejemplo no está influenciado por el muestreo de otro.\n",
    "* La distribución es estacionaria. Es decir no cambia a lo largo del conjunto de datos.\n",
    "* Los ejemplos son muestreados desde particiones de la misma distribución. Es decir, no se deben crear nuevas características en la partición de prueba.\n",
    "\n",
    "Adicionalmente, debemos tener en cuenta que se conserve la distribución de las etiquetas de los datos tanto en entrenamiento como en prueba (estratificación). En la siguiente sesión se va a estudiar en más detalle los efectos de hacer una partición estratificada. Scikit-Learn nos permite particionar un conjunto de datos en entrenamiento y prueba. A continuación vamos a dividir el conjunto en $70\\%$ para entrenamiento y $30\\%$ para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parámetros:\n",
    "\n",
    "* `test_size`: Tamaño de la partición de prueba\n",
    "* `random_state`: Semilla del generador de números pseudoaleatorios. Este parámetro garantiza reproducibilidad del particionamiento.\n",
    "* `stratify`: Si se estratifican los datos con respecto a `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verificar el número de muestras de ambas particiones y la distribución de clases de cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Número de muestras en entrenamiento: {}'.format(X_train.shape[0]))\n",
    "print('Número de muestras en prueba: {}'.format(X_test.shape[0]))\n",
    "print('Número de características: {}'.format(X_train.shape[1]))\n",
    "\n",
    "print('Distribución de clases en entrenamiento: {}'.format(np.bincount(y_train)))\n",
    "print('Distribución de clases en prueba: {}'.format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la partición recién creada y analizamos un modelo entrenado con $k = 200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=200)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6)) \n",
    "plot_decision_region(X_train, gen_pred_fun(knn))\n",
    "plot_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error en entrenamiento: {}'.format(1-knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que el error en entrenamiento es del $14\\%$. Además se evidencia que el modelo entrenado es ahora demasiado **simple** y no se puede ajustar la estructura de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora medimos el error de generalización del modelo entrenado y visualizamos la clasificación de los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))   \n",
    "plot_decision_region(X_test, gen_pred_fun(knn))\n",
    "plot_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error de generalización: {}'.format(1 - knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que cuando aumentamos el número de vecinos, nuestro modelo sufre de **subajuste**. La superficie de decisión se suaviza, pero no logra captar los detalles de los datos. Mientras el error de entrenamiento se acerca a $14\\%$, el error de generalización se acerca a $10\\%$.\n",
    "\n",
    "**¿Cómo estimar un buen número de $k$-vecinos más cercanos de manera que el modelo no sobreajuste ni subajuste los datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de la complejidad\n",
    "\n",
    "Un modelo de aprendizaje de máquina puede ser tan complejo como para recordar las particularidades y el ruido del conjunto de entrenamiento (**sobreajuste**), así como puede ser demasiado flexible para no modelar la variabilidad de los datos (**subajuste**). El modelo debe garantizar un compromiso entre el sobreajuste y el subajuste, lo cual se logra evaluando la complejidad del modelo. Una forma de evaluar la complejidad es analizar el error de entrenamiento y generalización para diferentes modelos que varían en su complejidad. En el caso de `KNearestNeighbor`, la complejidad está determinada por el número de vecinos $k$. **Entre menor sea el número de vecinos, más complejo es el modelo.**\n",
    "\n",
    "A continuación exploramos un conjunto de valores $k$, con el objetivo de encontrar aquel modelo con el mejor compromiso entre error de entrenamiento y error de generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(1, 20))\n",
    "print(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar un nuevo conjunto de datos con $1000$ muestras y haremos una partición $70-30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.4, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el error de entrenamiento y generalización con respecto al aumento de complejidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = []\n",
    "generalization_error = []\n",
    "\n",
    "for nn in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=nn)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_error.append(1 - knn.score(X_train, y_train))\n",
    "    generalization_error.append(1 - knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos ambas curvas de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))\n",
    "\n",
    "pl.plot(k_values, train_error, label=\"Entrenamiento\")\n",
    "pl.plot(k_values, generalization_error, label=\"Generalización\")\n",
    "pl.xticks(k_values)\n",
    "pl.xlabel(\"k-vecinos\")\n",
    "pl.ylabel(\"Error\")\n",
    "#pl.gca().invert_xaxis()\n",
    "pl.arrow(13, 0.16, 0, -0.01, head_width=0.2, head_length=0.01, fc='k', ec='k')\n",
    "pl.text(15, 0.165, 'Punto de balance')\n",
    "pl.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que el error de entrenamiento y generalización tiene su punto de balance mínimo con $k=13$. Observamos tambien que cuando el modelo es demasiado complejo ($k=1$), el error de generalización sube, así como el de entrenamiento cae a $0\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de Decisión\n",
    "\n",
    "A continuación, presentamos otro algoritmo de clasificación no lineal basado en árboles de decisión. Los árboles de decisión son muy intuitivos, puesto que codifican una serie de elecciones \"**si esto**\" o \"**sino entonces**\", de forma muy similar a como una persona tomaría una decisión. La gran ventaja de esta técnica es que estas elecciones se pueden aprender de forma automática desde los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo\n",
    "\n",
    "Considere el siguiente árbol de decisión. Este árbol de decisión describe una serie de elecciones que buscan determinar si espero (**V**) o no (**F**) por una mesa en un restaurante.\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/decisiontree2.png\" width=\"70%\">\n",
    "\n",
    "Con base al anterior árbol de decisión, puedo tomar la decisión de si espero o no, usando unas reglas de clasificación sencillas, por ejemplo:\n",
    "\n",
    "* **Si** Clientes = \"Lleno\" **Y** EsperaEstimada = \"10-30\" **Y** Hambre = \"No\" **Entonces** Esperar=\"SI\"\n",
    "* **Si** Clientes = \"Algunos\" **Entonces** Esperar=\"SI\"\n",
    "* **Si** Clientes = \"Lleno\" **Y** EsperaEstimada = \">60\" **Entonces** Esperar=\"NO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beneficios\n",
    "\n",
    "* Los datos de entrada requieren muy poco preprocesamiento. Los árboles de decisión pueden trabajar con variables de diferente tipo (continuas y variables) y son invariantes al escalamiento de las características. \n",
    "* Los modelos son fáciles de interpretar, los árboles pueden ser visualizados.\n",
    "* El costo computacional del uso del árbol para predecir la categoría de un ejemplo es mínimo comparado con otras técnicas (Tiempo logaritmico).\n",
    "\n",
    "## Contras\n",
    "\n",
    "* Puede ser tan complejo, que se memoriza el conjunto de datos, por lo tanto no generaliza tan bien (**Sobreajuste**).\n",
    "* Son muy sensibles al imbalance de clases (**Sesgo**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo se construye el árbol? - Algoritmo básico\n",
    "* El árbol es construido de arriba hacia abajo recursivamente de forma divide y vencerás.\n",
    "* Al comienzo, todos los ejemplos de entrenamiento están en la raíz.\n",
    "* Los atributos son categóricos (en caso de atributos continuos, se discretizan por adelantado)\n",
    "* Los ejemplos son repartidos recursivamente de acuerdo con el atributo seleccionado. \n",
    "* Los atributos de prueba son seleccionados en base a una heurística o medida estadística (ej. **ganancia de información**)\n",
    "* Se detiene hasta que solo hayan ejemplos de una clase en cada nodo hoja o se haya alcanza la profundidad máxima.\n",
    "\n",
    "## ¿Cómo seleccionar un atributo? ¿Cómo medir si una partición es buena?\n",
    "\n",
    "Una partición ideal es aquella que divide en un nodo las muestras de una misma clase. Observemos qué pasa si usamos la variable **Cliente** para particionar nuestro conjunto de datos.\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/split_clients.png\" width=\"50%\">\n",
    "Ahora, observemos qué pasa cuando particionamos el conjunto de datos usando la variable **Tipo de restaurante**.\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/split_type.png\" width=\"50%\">\n",
    "\n",
    "**¿Cuál variable es mejor?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación en Scikit-Learn\n",
    "\n",
    "La implementación de Scikit-Learn se consigue con la clase `DecisionTreeClassifier`. Cargamos de nuevo nuestro conjunto de datos usando IRIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:,[0, 2]] # clases: versicolor y virginica\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DecisionTreeClassifier` soporta varios parámetros como lo son:\n",
    "* `max_depth`: Profundidad máxima del árbol.\n",
    "* `criterion`: Medida para determinar la calidad del particionamiento generado por un atributo. Soporta coeficiente GINI y entropía.\n",
    "* `min_samples_split`: Controla el número mínimo de muestras que debe haber en un nodo luego de una partición.\n",
    "* `min_samples_leaf`: Controla el número mínimo de muestras que debe haber en un nodo hoja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos la superficie de decisión del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))    \n",
    "plot_decision_region(X, gen_pred_fun(classifier))\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error: {}'.format(1-classifier.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización\n",
    "\n",
    "El árbol de decisión aprendido puede ser visualizado usando `graphviz`. En Ubuntu, se recomienda instalarlo usando ambas líneas:\n",
    "* `conda install python-graphviz` o `conda install graphviz`\n",
    "* `sudo apt-get install graphviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a usar el conjunto de datos IRIS completo (Usando las cuatro características) y entrenamos un árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos `graphviz` para visualizar el árbol generado. `graphviz` soporta como parámetros los nombres de las clases y de las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(classifier, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importancia de las variables\n",
    "\n",
    "Una de las ventajas de usar árboles de decisión, es que nos permite determinar la importancia de cada características, con base al índice de impureza usado. Scikit-Learn nos permite acceder a la importancia de cada característica llamando `.feature_importances_`. Esta importancia cuantifica qué tanto aporta cada característica a mejorar el desempeño del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*petal width (cm)*\" es la característica más importante con un $92\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la complejidad usando `DecisionTreeClassifier`\n",
    "\n",
    "Para evaluar la complejidad, vamos a estimar el número subóptimo de profundidad del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explorar los siguientes valores de profundidad máxima:\n",
    "* $[1, 2, 3, \\dots, 20]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = []\n",
    "generalization_error = []\n",
    "max_depth_values = list(range(1, 21, 1))\n",
    "\n",
    "for depth in max_depth_values:\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=depth)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    train_error.append(1 - decision_tree.score(X_train, y_train))\n",
    "    generalization_error.append(1 - decision_tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos la curva de error de entrenamiento contra error de generalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize = (10, 6))\n",
    "\n",
    "pl.plot(max_depth_values, train_error, label=\"Entrenamiento\")\n",
    "pl.plot(max_depth_values, generalization_error, label=\"Generalización\")\n",
    "pl.xticks(max_depth_values)\n",
    "pl.xlabel(\"Profundidad máxima\")\n",
    "pl.ylabel(\"Error\")\n",
    "pl.arrow(2.2, 0.07, 0, -0.01, head_width=0.2, head_length=0.01, fc='k', ec='k')\n",
    "pl.text(2, 0.08, 'Punto de balance')\n",
    "pl.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuación del parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo con el conjunto de datos **Wine**.\n",
    "\n",
    "# Realice:\n",
    "\n",
    "* Cargue el conjunto de datos **Wine**.\n",
    "* Genere una partición estratificada 70-30 sobre el conjunto de datos.\n",
    "* Genere una gráfica de barras en Pandas que verifique que las particiones de entrenamiento y prueba tengan la misma distribución de clases.\n",
    "\n",
    "## Explorando la complejidad usando `KNearestNeighbor`\n",
    "* Entrene un modelo `KNearestNeighbor`. Use los siguientes valores para evaluar la complejidad:\n",
    "    * $[1, 2, 3, \\dots, 20]$\n",
    "    * Grafique los errores de entrenamiento y generalización conforme a la complejidad el modelo aumenta.\n",
    "* Construya un conjunto de datos usando las características `Proline` contra `Flavonoids`. \n",
    "    * Use la partición 70-30 definida al inicio\n",
    "    * Usando `KNearestNeighbor`, determine el número subóptimo de k-vecinos usando evaluación de la complejidad. Use los mismos valores de $k$: $[1, 2, 3, \\dots, 20]$\n",
    "    * Grafique la superficie de decisión contra los ejemplos de test.\n",
    "    * Reporte accuracy, el error de clasificación, la precisión macro, el recall macro y el F1 score macro sobre el **conjunto de prueba**.\n",
    "\n",
    "## Explorando la complejidad usando `DecisionTree`\n",
    "* Entrene un modelo de árbol de decisión usando los siguientes valores de profundidad:\n",
    "    * $[1, 2, 3, 4, 5, 7, 8, 9, 10]$\n",
    "    * Grafique los errores de entrenamiento y generalización conforme a la complejidad el modelo aumenta.\n",
    "* Construya un conjunto de datos usando las características `Alcalinity` contra `Malic Acid`.\n",
    "    * Use la partición 70-30 definida al inicio\n",
    "    * Usando `DecisionTree`, determine el número subóptimo de profundidad máxima usando evaluación de la complejidad. Use los mismos valores de $\\textit{max_depth}$: $[1, 2, 3, 4, 5, 7, 8, 9, 10]$\n",
    "    * Grafique la superficie de decisión contra los ejemplos de test.\n",
    "    * Reporte accuracy, el error de clasificación, la precisión macro, el recall macro y el F1 score macro sobre el **conjunto de prueba**.\n",
    "\n",
    "## Visualización usando `DecisionTree`\n",
    "* Escoja el mejor modelo entrenado sobre las 13 características. Visualice el árbol de decisión.\n",
    "* ¿Cuales son las características más importantes del modelo?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
