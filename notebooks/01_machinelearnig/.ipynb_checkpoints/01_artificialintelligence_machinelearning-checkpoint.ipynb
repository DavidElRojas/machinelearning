{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<a href=\"https://colab.research.google.com/github/marcoteran/machinelearning/blob/master/notebooks/01_machinelearning/01_artificialintelligence_machinelearning.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de código\n",
    "# Sesión 01: Clasificación binaria usando un modelo lineal (Demo)\n",
    "## Inteligencia Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Marco Teran **E-mail:** marco.tulio.teran@gmail.com,\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KV50uBY7pPQC"
   },
   "source": [
    "El **Aprendizaje de Máquina** se concentra en la construcción y estudio de sistemas que puedan *aprender* de los datos. El problema principal consiste en *encontrar patrones, relaciones y regularidades* sobre los datos, los cuales le permitan construir modelos descriptivos y predictivos. Una aplicación muy común del Aprendizaje de Máquina consiste en la detección de *spam*, en el cual un modelo recibe un nuevo correo y lo etiqueta como spam o no.\n",
    "\n",
    "Para construir un modelo de este tipo, se deben tener en cuenta dos conceptos claves:\n",
    "- El primero es que el modelo debe debe realizar un **proceso automático de clasificación**, sin que el usuario le especifique de forma explicita la forma en la que la clasificación se lleve a cabo. *Por ejemplo*, el modelo recibe ejemplos de correos que son spam y ejemplos de correos que no lo son.\n",
    "- El segundo concepto consiste en que el modelo debe tener capacidad de **generalización**, es decir, el modelo debe ser capaz de predecir sobre datos nunca antes vistos. En el ejemplo del filtrado de spam, estamos interesados en *clasificar* de forma automática los correos que vayan llegando a la bandeja del usuario.\n",
    "\n",
    "A continuación abordaremos un problema de clasificación binaria sobre un subconjunto del conjunto de datos IRIS.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IdgjH40pPQD"
   },
   "source": [
    "# Un problema de clasificación de dos clases\n",
    "\n",
    "El siguiente código va a cargar un conjunto de datos [(IRIS)](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) que nos va a servir para ilustrar en que consiste un problema de clasificación y como resolverlo con un modelo de machine learning. Por ahora no nos vamos a preocupar de donde vienen los datos y como se procesan.\n",
    "\n",
    "### El conjunto de datos de Iris\n",
    "Este conjunto de datos consta de **3 tipos diferentes** de iris *[(Setosa, Versicolour y Virginica)](https://es.wikipedia.org/wiki/Conjunto_de_datos_flor_iris)* de longitud de pétalos y sépalos, almacenados en un ```numpy.ndarray``` de 150x4.\n",
    "- Las filas son las muestras y las columnas son: Longitud del sépalo, Ancho del sépalo, Longitud del pétalo y Ancho del pétalo.\n",
    "- El siguiente gráfico utiliza las dos primeras características. Consulte aquí para obtener más información sobre este conjunto de datos:\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_001.png\" width=\"40%\" />\n",
    "  <img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_002.png\" width=\"40%\" /> \n",
    "    \n",
    "</p>\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/iris_dataset.png\" width=\"60%\"> \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1525815291208,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "gPS-NHDdpPQE",
    "outputId": "0eaef047-1cdb-4159-b6df-73317cbe97e1"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "def plot_data(X, y):\n",
    "    y_unique = np.unique(y)\n",
    "    colors = pl.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n",
    "    for this_y, color in zip(y_unique, colors):\n",
    "        this_X = X[y == this_y]\n",
    "        pl.scatter(this_X[:, 0], this_X[:, 1],  c=color.reshape(1,-1),\n",
    "                    alpha=0.5, edgecolor='k',\n",
    "                    label=\"Class %s\" % this_y)\n",
    "    pl.legend(loc=\"best\")\n",
    "    pl.title(\"Data\")\n",
    "\n",
    "y = 2*iris.target[iris.target != 0] - 3\n",
    "X_noscale = iris.data[:,[1, 3]]\n",
    "X_noscale = X_noscale[iris.target != 0, :]\n",
    "X = preprocessing.scale(X_noscale) # estándariza los datos eliminando la media y escalando los datos de forma que su varianza sea igual a 1\n",
    "\n",
    "\n",
    "pl.figure(figsize=(8, 6))\n",
    "pl.xlabel('Sepal Width')\n",
    "pl.ylabel('Petal Width')\n",
    "plot_data(X_noscale, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyogyjcrpPQJ"
   },
   "source": [
    "Los datos deben ser presentados como arreglos bi-dimensionales de números. Cada fila corresponde a una instancia de entrenamiento, sobre la cual queremos aprender o hacer una predicción sobre sus datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRpqE7S6pPQL"
   },
   "source": [
    "Verifiquemos el tamaño de cada arreglo construído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1525814170486,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "jz8VH_rwpPQL",
    "outputId": "d6f8544d-7760-403a-a952-4cf65ca15c14"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1525814176327,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "4w16Ae3cpPQQ",
    "outputId": "a0f6fab4-a6dc-4abd-b477-4d5874ddbff7"
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(list(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hNob0J7pPQT"
   },
   "source": [
    "Observamos que cada clase tiene 50 ejemplos en total. los datos de una clase se representan con una etiqueta positiva, +1, y los de la otra clase con una etiqueta positiva, +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1525815295391,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "3h962sAfrXnr",
    "outputId": "2db0dbfc-7f9a-4f0d-90d9-e11d730545e4"
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7A85yN-vpPQU"
   },
   "source": [
    "## Pregunta\n",
    "**¿Cómo diferenciamos entre ambas clases de manera automática?**\n",
    "\n",
    "Es decir si me dan un nuevo ejemplo, quiero poderlo clasificar en uno de estas dos clases.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoJ3wl4BpPQU"
   },
   "source": [
    "# Clasificación usando un modelo lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWvUeu1CpPQV"
   },
   "source": [
    "## Discriminación lineal\n",
    "\n",
    "* Nuestro modelo de clasificación es una función que recibe un ejemplo $x$ y retorna la predicción. Esta función se basa en una función (llamada discriminante) $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ tal que:\n",
    "$$\\textrm{Predicción}(x)=\\begin{cases}\n",
    "C_{1} & \\mbox{si }f(x)\\ge \\theta\\\\\n",
    "C_{2} & \\mbox{si }f(x)<\\theta\n",
    "\\end{cases}$$\n",
    "\n",
    "* Para el caso de discriminación lineal, definimos $f$ como un modelo lineal con parámetros $w$ y $w_0$:\n",
    "$$f(x) = P(C_1|x)= wx+w_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayzdZfmmpPQW"
   },
   "source": [
    "### Problemas\n",
    "\n",
    "* **¿Cómo encontrar $f$?**\n",
    "\n",
    "* **¿Cómo estimamos $w$ y $w_0$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZTD8EcZpPQW"
   },
   "source": [
    "## Funciones de pérdida\n",
    "\n",
    "La pérdida (loss en inglés) es la penalidad por una predicción errada. En otras palabras, la pérdida indica qué tan errada fue la predicción de un modelo en un ejemplo. Si la predicción de mi modelo es perfecta, la pérdida es **cero**, de lo contrario, la pérdida va a ser un número mayor a cero.\n",
    "\n",
    "Una función de pérdida muy común es la pérdida cuadrática:\n",
    "$$ L(f, D) =\\sum_{(x_{i},r_{i})\\in D} (r_i - f(x_i))^2 $$\n",
    "\n",
    "donde:\n",
    "* $(x_{i},r_{i})$ es un ejemplo en el cual $x_i$ corresponde a un conjunto de características. En nuestro conjunto de datos, corresponde a las 2 características extraídas de una flor (Ancho del sépalo y ancho del pétalo). Estas características son usadas por el modelo para hacer predicciones. $r_i$ corresponde a la etiqueta del ejemplo, por ejemplo la especie de la flor.\n",
    "* $f(x_i)$ corresponde a la función de predicción que definimos previamente. Esta función es de la forma $f(x) = wx+w_0$.\n",
    "* $D$ corresponde al conjunto de datos compuesto por varios ejemplos anotados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjiSUzEBpPQX"
   },
   "source": [
    "## Aprendizaje como optimización\n",
    "\n",
    "* Estimar los parámetros $w$ y $w_0$ puede ser abordado como un problema de optimización que consiste en:\n",
    "$$\\min_{f\\in H}L(f,D)$$\n",
    "dónde $L(f, D)$ es la función de pérdida cuadrática. Esto se resume en encontrar una función $f$ que genere el valor mínimo de pérdida promedio con respecto a todos los ejemplos del conjunto de datos.\n",
    "* La función $f$ proviene de una conjunto de funciones llamado el espacio de hipótesis:\n",
    "$$H=\\{f_w(x,y)=wx+w_0,\\forall w\\in\\mathbb{R}^n \\ y \\ w_0\\in\\mathbb{R}\\}$$\n",
    "dónde $w$ y $w_0$ son los coeficientes de la función $f(x) = wx+w_0$.\n",
    "* La función de pérdida nos ayuda a estimar qué tan mal se comporta una función $f$ del espacio de hipótesis con respecto al conjunto de datos $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jx_jZfKepPQY"
   },
   "source": [
    "A continuación, definimos en Python las funciones de predicción y pérdida, para nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YJk6hCAxpPQZ"
   },
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    a = np.dot(w[1:], x) + w[0]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8FufsqudpPQb"
   },
   "outputs": [],
   "source": [
    "def square_loss(w, x, y):\n",
    "    return (y - predict(w, x)) ** 2 / 2\n",
    "\n",
    "def batch_loss(loss_fun, w, X, Y):\n",
    "    n = X.shape[0]\n",
    "    tot_loss = 0\n",
    "    for i in range(n):\n",
    "        tot_loss += loss_fun(w, X[i], Y[i])\n",
    "    return tot_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyudE9oZpPQe"
   },
   "source": [
    "Supongamos $w$ y $w_0$:\n",
    "* $w = [5, 2]$\n",
    "* $w_0 = 1$\n",
    "\n",
    "Escogemos el primer ejemplo de nuestro conjunto de datos Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1525815325387,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "lXWsUOdYpPQf",
    "outputId": "37d22d3f-19b9-48fe-d6d1-471766ce6b51"
   },
   "outputs": [],
   "source": [
    "w = [1, 5, 2] # Por facilidad, w[0] es igual a w_0\n",
    "x = X[0] # Primer ejemplo de nuestro conjunto de datos IRIS.\n",
    "label = y[0]\n",
    "\n",
    "print('Características: {}'.format(X[0]))\n",
    "print('Etiqueta real: {}'.format(y[0]))\n",
    "print('Función discriminante: {}'.format(predict(w, x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kksP6jNtpPQi"
   },
   "source": [
    "Puesto que el valor de la función discriminante es $f(x)>0$, la clase predicha es positiva.\n",
    "Ahora, estimamos el valor de pérdida usando esos valores de $w$ y $w_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1525815362923,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "DZZe6JFcpPQj",
    "outputId": "a52c6ae4-7cde-4f94-a521-fff7e02668b8"
   },
   "outputs": [],
   "source": [
    "print('Pérdida: {}'.format(square_loss(w, x, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03AgoXXLpPQm"
   },
   "source": [
    "¿Qué pasa si modificamos uno de los parámetros $w$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1525815387567,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "VkOJEGWnpPQn",
    "outputId": "197fa001-48e3-48b4-ef07-12d20932e568"
   },
   "outputs": [],
   "source": [
    "w_p = [1, 5, 3]\n",
    "\n",
    "print('Etiqueta: {}'.format(y[0]))\n",
    "print('Predicción: {}'.format(predict(w_p, x)))\n",
    "print('Pérdida w_p: {}'.format(square_loss(w_p, x, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNLN23qwpPQu"
   },
   "source": [
    "Observamos que nuestra pérdida bajó de $15.94$ a $12.47$. Sin embargo, estamos interesados en automatizar este proceso. Primero vamos a observar cómo se comporta la función de pérdida cuadrática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u92KZOcNpPQy"
   },
   "source": [
    "## Analizando la función de pérdida cuadrática\n",
    "\n",
    "A continuación, definimos un par de funciones de python para poder visualizar la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PGJ6rUUEpPQz"
   },
   "outputs": [],
   "source": [
    "def plot_loss(loss):\n",
    "    w1_vals = np.linspace(-5, 5, 30)\n",
    "    w2_vals = np.linspace(-5, 5, 30)\n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    grid_r, grid_c = W1.shape\n",
    "    ZZ = np.zeros((grid_r, grid_c))\n",
    "    for i in range(grid_r):\n",
    "        for j in range(grid_c):\n",
    "            ZZ[i, j] = loss(W1[i, j], W2[i, j])\n",
    "    pl.contourf(W1, W2, ZZ,30, cmap = pl.cm.jet)\n",
    "    pl.colorbar()\n",
    "    pl.xlabel(\"w1\")\n",
    "    pl.ylabel(\"w2\")\n",
    "\n",
    "def bloss_square(w1, w2):\n",
    "    w = np.array([1, w1, w2])\n",
    "    return batch_loss(square_loss, w, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1273,
     "status": "ok",
     "timestamp": 1525815442513,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "x2ad9mr3pPQ3",
    "outputId": "62c4f2fd-11f8-4d23-8b27-85c430798ad3"
   },
   "outputs": [],
   "source": [
    "plot_loss(bloss_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nExl8BeBpPQ7"
   },
   "source": [
    "Intepretación: Observamos que el valor mínimo de nuestra función de pérdida se encuentra alrededor de:\n",
    "* $w_1 = 0.0$\n",
    "* $w_2 = 1.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VXyDto7pPQ9"
   },
   "source": [
    "## ¿Cómo resolver el problema de aprendizaje?\n",
    "\n",
    "* Existen varios enfoques:\n",
    "    * Optimización lineal\n",
    "    * Optimización convexa\n",
    "    * Optimización no-lineal\n",
    "    * Optimización combinatoria\n",
    "* No existe una estrategia de optimización que funcione para todos los problemas. [No free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)\n",
    "* **En este caso vamos a usar gradiente descendente** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAHCFSE7pPQ9"
   },
   "source": [
    "## Gradiente descendente\n",
    "\n",
    "* Ventajas:\n",
    "  * Óptimo global garantizado\n",
    "  * Simplicidad del método\n",
    "  * Facilidad para ajustar los parámetros\n",
    "  * Escalabilidad\n",
    "  * Paralelización potencial.\n",
    "* En aprendizaje de máquina, las preferencias cambian sobre el tiempo.\n",
    "* Hoy en día es escalable. Inclusive, la estrategias paralelizables son preferidas a sabiendas de la optimalidad garantizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tNQ0G_spPRC"
   },
   "source": [
    "Minimizar la función $L(f, D)$, donde $f$ es nuestra función de predicción resulta ser un problema convexo, es decir, existe solo un lugar donde la inclinación de la curva de pérdida es igual a cero. Ese valor mínimo es dónde nuestra función de pérdida $L$ **converge**.\n",
    "\n",
    "Por otro lado, calcular el valor de la función de pérdida para cada posible valor de $w$ sobre el conjunto de datos es una forma muy ineficiente de hallar ese valor de convergencia. Gradiente descendente es un mecanismo iterativo que nos permite hayar ese valor de convergencia.\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/gradient_on_surface.png\" width=\"60%\">\n",
    "\n",
    "### Algoritmo gradiente descendente:\n",
    "\n",
    "* Paso 1: Escogemos un valor inicial o punto de partida. Puede ser cero o un valor aleatorio. Este valor inicial no tiene relevancia para este problema. \n",
    "* Paso 2: Calculamos el **gradiente de la función de pérdida** para el punto de partida. El gradiente de la pérdida me indica la derivada (inclinación) de la curva. El gradiente a su vez es un vector, por lo tanto tiene **dirección** y **magnitud**, esta dirección siempre apunta hacía donde se genera el mayor incremento en la función de pérdida. \n",
    "* Paso 3: Calculamos un nuevo valor $w$ cambiándolo en la dirección negativa del gradiente, con el objetivo reducir la pérdida lo más pronto posible.\n",
    "* Paso 4: El nuevo valor de $w$ también estará determinado por la magnitud del gradiente y una **taza de aprendizaje** $\\eta$. Por ejemplo, si la magnitud del gradiente es 2.5 y la taza de aprendizaje es 0.01, gradiente descendente escogerá un punto $w$ que esté alejado 0.025 unidades del punto anterior $w$. Para el calculo de $w$ usaremos la siguiente formula:\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "* Repetir desde el Paso 1.\n",
    "\n",
    "#### Problemas\n",
    "\n",
    "* ¿Qué pasa si el tamaño de nuestro paso es muy grande y no alcanza el minimo local?\n",
    "\n",
    "<img src=\"https://github.com/marcoteran/machinelearning/raw/master/notebooks/01_machinelearnig/figures/lr.png\" width=\"60%\">\n",
    "\n",
    "La elección de una tasa de aprendizaje alta puede terminar en que nunca alcanzemos el punto de convergencia. Pero una tasa muy pequeña puede llevar a que tome mucho tiempo llegar al mínimo global\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fi5JVw_8pPRD"
   },
   "source": [
    "### ¿Cómo se calcula el gradiente?\n",
    "\n",
    "$$\n",
    "\\begin{aligned}L(f,D) & = \\sum_{(x_{i},r_{i})\\in D} (r_i - f(x_i))^2 \\\\\n",
    " & =\\sum_{(x_{i},r_{i})\\in D}E(w, x_i, r_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Si $f_w$ es la función que definimos antes:\n",
    "$$\n",
    "\\frac{\\partial E(w,x_{i,}r_{i})}{\\partial w}=(f_{w}(x_{i})-r_{i})x_{i}\n",
    "$$\n",
    "\n",
    "A continuación, definimos una función `de_dw` que corresponde el derivadas parciales de $E$ con respecto a los coeficientes $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Hg4_lVtGpPRD"
   },
   "outputs": [],
   "source": [
    "def de_dw(w, x, r):\n",
    "    x_prime = np.zeros(len(x) + 1)\n",
    "    x_prime[1:] = x\n",
    "    x_prime[0] = 1\n",
    "    return (predict(w, x) - r) * x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8OCEAWDpPRG"
   },
   "source": [
    "## Gradiente descendente en batch\n",
    "\n",
    "Para estimar el gradiente, lo hacemos a lo largo de todos nuestro conjunto de datos. Puesto que solo contamos con 100 ejemplos, es relativamente rápido hacer este calculo. Sin embargo, en la pŕactica puede ser ineficiente si se cuenta con un gran número de ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UnThWOsTpPRH"
   },
   "outputs": [],
   "source": [
    "def batch_gd(X, Y, epochs, eta, w_ini):\n",
    "    \"\"\"\n",
    "    X: instancias del conjunto de datos\n",
    "    Y: etiquetas del conjunto de datos\n",
    "    epochs: número de iteraciones para ejecutar gradiente descendente\n",
    "    eta: taza de aprendizaje\n",
    "    w_ini: w y w_0 iniciales\n",
    "    \n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w = w_ini \n",
    "    n = X.shape[0]\n",
    "    for i in range(epochs): \n",
    "        delta = np.zeros(len(w))\n",
    "        for j in range(n):\n",
    "            delta += de_dw(w, X[j], Y[j]) # Vamos sumando el gradiente por cada ejemplo en el conjunto de datos\n",
    "        w = w - eta * delta # Calculamos el nuevo valor de w\n",
    "        losses.append(batch_loss(square_loss, w, X, Y)) # Vamos guardando el valor de pérdida para visualizar luego\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBnrPOD3pPRK"
   },
   "source": [
    "Vamos a ejecutar gradiente descendente en batch con los siguientes parámetros:\n",
    "* $epochs = 50$\n",
    "* $w_0 = 0$ ($w_0$ inicial)\n",
    "* $w = [0, 0]$ ($w$ inicial)\n",
    "* $\\eta = 0.01$ (tasa de aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1525816190012,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "_HpU2WjbpPRL",
    "outputId": "6cb03d79-5fac-4d48-cd05-065eaffdbdb0"
   },
   "outputs": [],
   "source": [
    "w, losses = batch_gd(X, y, 50, 0.01, np.array([0, 0, 0]))\n",
    "pl.figure(figsize = (8,16/3))\n",
    "pl.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpdS5rPqpPRO"
   },
   "source": [
    "# Gradiente descendente estocástico (SGD)\n",
    "\n",
    "\n",
    "Cuando trabajamos con grandes conjuntos de datos, se vuelve poco práctico el calculo del gradiente sobre todo el conjunto de datos. Una forma de disminuir el tiempo de computación es escogiendo muestras al azar de nuestro conjunto de datos, que nos generan un estimado en promedio del gradiente promedio. SGD toma entonces una muestra de forma aleatoria a la vez y estima el valor del gradiente para esa muestra. A pesar de ser ruidoso, SGD funciona bastante bien en la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oJExhoRLpPRQ"
   },
   "outputs": [],
   "source": [
    " def sgd(X, Y, epochs, eta, w_ini):\n",
    "    \"\"\"\n",
    "    X: instancias del conjunto de datos\n",
    "    Y: etiquetas del conjunto de datos\n",
    "    epochs: número de iteraciones para ejecutar\n",
    "            gradiente descendente\n",
    "    eta: tasa de aprendizaje\n",
    "    w_ini: w y w_0 iniciales\n",
    "    \n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    w = w_ini\n",
    "    n = X.shape[0]\n",
    "    for i in range(epochs):\n",
    "        for j in range(n):\n",
    "            delta = de_dw(w, X[j], Y[j]) # Aquí estimamos el gradiente para cada elemento pero no para todo el dataset\n",
    "            w = w - eta * delta\n",
    "        losses.append(batch_loss(square_loss, w, X, Y))\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DONCNgbRpPRZ"
   },
   "source": [
    "A continuación, comparamos el comportamiento de Gradiente Descendente en Batch y Estocástico. Observamos que tienen un comportamiento similar para nuestro problema de clasificación de dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1525816534571,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "2E2G5xfbpPRZ",
    "outputId": "3286ec27-ccef-4bcc-ebc1-5bd0d4393f9f"
   },
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "epochs = 500\n",
    "w1, losses_bt = batch_gd(X, y, epochs, lr, np.array([0, 0, 0])) #Batch\n",
    "w2, losses_ol = sgd(X, y, epochs, lr, np.array([0, 0, 0])) #SGD\n",
    "pl.figure(figsize = (8,16/3))\n",
    "pl.plot(np.arange(epochs), losses_ol, label=\"SGD\")\n",
    "pl.plot(np.arange(epochs), losses_bt, label=\"Batch\")\n",
    "pl.xlabel(\"Epoch\")\n",
    "pl.ylabel(\"Loss\")\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utjBLJqYpPRc"
   },
   "source": [
    "## Visualización de la función de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_sWueJlpPRc"
   },
   "source": [
    "Es útil poder visualizar qué regiones de nuestro espacio en 2D son asignadas a la clase positiva y a la clase negativa. Para eso escribimos una función que nos permite visualizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WRLsCq20pPRd"
   },
   "outputs": [],
   "source": [
    "def plot_decision_region(X, pred_fun):\n",
    "    \"\"\"\n",
    "    X: corresponde a las instancias de nuestro conjunto de datos\n",
    "    pred_fun: es una función que para cada valor de X, me regresa una predicción\n",
    "    \"\"\"\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "    min_x = min_x - (max_x - min_x) * 0.05\n",
    "    max_x = max_x + (max_x - min_x) * 0.05\n",
    "    min_y = min_y - (max_y - min_y) * 0.05\n",
    "    max_y = max_y + (max_y - min_y) * 0.05\n",
    "    x_vals = np.linspace(min_x, max_x, 30)\n",
    "    y_vals = np.linspace(min_y, max_y, 30)\n",
    "    XX, YY = np.meshgrid(x_vals, y_vals)\n",
    "    grid_r, grid_c = XX.shape\n",
    "    ZZ = np.zeros((grid_r, grid_c))\n",
    "    for i in range(grid_r):\n",
    "        for j in range(grid_c):\n",
    "            ZZ[i, j] = pred_fun(XX[i, j], YY[i, j])\n",
    "    pl.contourf(XX, YY, ZZ, 30, cmap = pl.cm.coolwarm, vmin= -1, vmax=2)\n",
    "    pl.colorbar()\n",
    "    pl.xlabel(\"x\")\n",
    "    pl.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "U5SXcSgXpPRf"
   },
   "outputs": [],
   "source": [
    "def gen_pred_fun(w):\n",
    "    def pred_fun(x1, x2):\n",
    "        x = np.array([x1, x2])\n",
    "        return predict(w, x)\n",
    "    return pred_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HymZ1v_-pPRh"
   },
   "source": [
    "Visualizemos el caso donde nuestros coeficiente sean de la forma:\n",
    "* $w_0 = 0$\n",
    "* $w = [0.6, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1525816583497,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "ksaA3NBzpPRi",
    "outputId": "951ce054-d242-46de-8272-21e2e961d870"
   },
   "outputs": [],
   "source": [
    "w = [0, 0.6, 1]\n",
    "\n",
    "pl.figure(figsize = (8,16/3))    \n",
    "plot_decision_region(X, gen_pred_fun(w))\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hDsvK5ipPRl"
   },
   "source": [
    "Nada útil, sin embargo si visualizamos el resultado luego de aplicar SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684,
     "status": "ok",
     "timestamp": 1525816599579,
     "user": {
      "displayName": "Fabio A. Gonzalez",
      "photoUrl": "//lh5.googleusercontent.com/-jcmRx3yeVv4/AAAAAAAAAAI/AAAAAAAAAic/QZe3kSPTExE/s50-c-k-no/photo.jpg",
      "userId": "112315845334496218213"
     },
     "user_tz": 300
    },
    "id": "qP3ch_fHpPRm",
    "outputId": "968817ec-f336-4b42-a17f-e9c0d46dfa14"
   },
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "epochs = 500\n",
    "w, _ = sgd(X, y, epochs, lr, np.array([0, -1, 2])) #SGD\n",
    "\n",
    "pl.figure(figsize = (8,16/3))    \n",
    "plot_decision_region(X, gen_pred_fun(w))\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_O1ROxMZpPRp",
    "outputId": "0e5c7dc4-cfcd-47c8-96cd-b545ba85f716"
   },
   "outputs": [],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "s1_machine_learning.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
